{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG Verification Agent\n",
                "\n",
                "This notebook implements a RAG Verification Agent that retrieves information from `report.pdf` and verifies/supplements it with web search using Tavily."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "1d1983db",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "import chromadb\n",
                "from pypdf import PdfReader\n",
                "from tavily import TavilyClient\n",
                "from google import genai\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
                "if not GOOGLE_API_KEY:\n",
                "    GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
                "\n",
                "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
                "\n",
                "if not GOOGLE_API_KEY:\n",
                "    raise ValueError(\"GOOGLE_API_KEY or GEMINI_API_KEY not found in environment variables\")\n",
                "if not TAVILY_API_KEY:\n",
                "    raise ValueError(\"TAVILY_API_KEY not found in environment variables\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "afd08308",
            "metadata": {},
            "source": [
                "## 1. Initialize Clients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "1a0430f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Gemini Client\n",
                "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
                "\n",
                "# Initialize Tavily Client\n",
                "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
                "\n",
                "# Initialize ChromaDB Client\n",
                "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
                "collection_name = \"pdf_verification\"\n",
                "\n",
                "# Reset collection if needed for fresh ingestion\n",
                "try:\n",
                "    chroma_client.delete_collection(name=collection_name)\n",
                "except Exception:\n",
                "    pass # Collection didn't exist or other error\n",
                "\n",
                "collection = chroma_client.create_collection(name=collection_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e452a277",
            "metadata": {},
            "source": [
                "## 2. Ingest PDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67921394",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 2 chunks.\n",
                        "Generating embeddings and storing in ChromaDB...\n",
                        "Ingestion complete.\n"
                    ]
                }
            ],
            "source": [
                "def extract_text_from_pdf(pdf_path):\n",
                "    reader = PdfReader(pdf_path)\n",
                "    text = \"\"\n",
                "    for page in reader.pages:\n",
                "        text += page.extract_text()\n",
                "    return text\n",
                "\n",
                "def chunk_text(text, chunk_size=1000, overlap=200):\n",
                "    chunks = []\n",
                "    start = 0\n",
                "    while start < len(text):\n",
                "        end = start + chunk_size\n",
                "        chunks.append(text[start:end])\n",
                "        start += chunk_size - overlap\n",
                "    return chunks\n",
                "\n",
                "# Load and Chunk\n",
                "pdf_path = \"Stanford.pdf\"\n",
                "if not os.path.exists(pdf_path):\n",
                "    print(f\"File {pdf_path} not found. Please ensure Standford.pdf is in the directory.\")\n",
                "else:\n",
                "    pdf_text = extract_text_from_pdf(pdf_path)\n",
                "    chunks = chunk_text(pdf_text)\n",
                "    print(f\"Created {len(chunks)} chunks.\")\n",
                "\n",
                "    print(\"Generating embeddings and storing in ChromaDB...\")\n",
                "    documents = []\n",
                "    embeddings = []\n",
                "    ids = []\n",
                "\n",
                "    # Batching could be done here, but for simplicity we loop\n",
                "    for i, chunk in enumerate(chunks):\n",
                "        # Embed the chunk using Gemini\n",
                "        response = client.models.embed_content(\n",
                "            model=\"text-embedding-004\",\n",
                "            contents=chunk\n",
                "        )\n",
                "        # The new SDK returns an object with embeddings list\n",
                "        embedding = response.embeddings[0].values\n",
                "        \n",
                "        documents.append(chunk)\n",
                "        embeddings.append(embedding)\n",
                "        ids.append(f\"chunk_{i}\")\n",
                "\n",
                "    collection.add(\n",
                "        documents=documents,\n",
                "        embeddings=embeddings,\n",
                "        ids=ids\n",
                "    )\n",
                "    print(\"Ingestion complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "88921d31",
            "metadata": {},
            "source": [
                "## 3. Web Search Tool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "25425bd4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def search_web(query):\n",
                "    try:\n",
                "        response = tavily_client.search(query, search_depth=\"basic\")\n",
                "        results = response.get(\"results\", [])\n",
                "        context = \"\"\n",
                "        for result in results:\n",
                "            context += f\"Source: {result['url']}\\nContent: {result['content']}\\n\\n\"\n",
                "        return context\n",
                "    except Exception as e:\n",
                "        return f\"Error performing web search: {e}\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b4ddf5f",
            "metadata": {},
            "source": [
                "## 4. Verification Agent Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2332d1f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def verify_information(user_query):\n",
                "    print(f\"Processing query: {user_query}\")\n",
                "    \n",
                "    # 1. Retrieve from PDF\n",
                "    print(\"Retrieving context from PDF...\")\n",
                "    query_embedding_resp = client.models.embed_content(\n",
                "        model=\"text-embedding-004\",\n",
                "        contents=user_query\n",
                "    )\n",
                "    query_embedding = query_embedding_resp.embeddings[0].values\n",
                "    \n",
                "    results = collection.query(\n",
                "        query_embeddings=[query_embedding],\n",
                "        n_results=3\n",
                "    )\n",
                "    \n",
                "    pdf_context = \"No relevant information found in PDF.\"\n",
                "    if results['documents'] and results['documents'][0]:\n",
                "        pdf_context = \"\\n---\\n\".join(results['documents'][0])\n",
                "        \n",
                "    # 2. Search Web\n",
                "    print(\"Searching web for verification/supplemental info...\")\n",
                "    web_context = search_web(user_query)\n",
                "    \n",
                "    # 3. Generate Answer\n",
                "    print(\"Generating final response...\")\n",
                "    prompt = f\"\"\"\n",
                "    You are a verification agent. Your goal is to answer the user's query using information from a provided PDF and verify/supplement it with information from the web.\n",
                "\n",
                "    User Query: {user_query}\n",
                "\n",
                "    Context from PDF (Internal Knowledge):\n",
                "    {pdf_context}\n",
                "\n",
                "    Context from Web Search (External Verification):\n",
                "    {web_context}\n",
                "\n",
                "    Instructions:\n",
                "    - Synthesize the information from both sources.\n",
                "    - Highlight if the PDF information is consistent with the web information.\n",
                "    - If there are discrepancies, point them out.\n",
                "    - If the PDF lacks information, rely on the web but state that the PDF did not contain the info.\n",
                "    - Provide a clear and concise answer.\n",
                "    \"\"\"\n",
                "    \n",
                "    response = client.models.generate_content(\n",
                "        model=\"gemini-2.5-flash-lite\",\n",
                "        contents=prompt,\n",
                "        config=genai.types.GenerateContentConfig(\n",
                "            temperature=0.3\n",
                "        ),\n",
                "    )\n",
                "    \n",
                "    return response.text"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9b5e7e3a",
            "metadata": {},
            "source": [
                "## 5. Run the Agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "c07cee4d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing query: Summarize the main points of the report and verify whether there is a document from Stanford which talks about context with high accuracy suddenly collapsing in size and its performance decreasing significantly\n",
                        "Retrieving context from PDF...\n",
                        "Searching web for verification/supplemental info...\n",
                        "Generating final response...\n",
                        "\n",
                        "=== AGENT RESPONSE ===\n",
                        "\n",
                        "The main point of the report, as extracted from the provided PDF context, describes a significant event where a system's context, initially at 8,282 tokens with 66.7% accuracy, suddenly collapsed to just 122 tokens during an update. This collapse resulted in a substantial drop in accuracy to 57.1%, which was notably worse than the 63.7% baseline. The remaining text in the PDF appears to be a poetic or personal reflection and does not contribute to the summary of a technical report.\n",
                        "\n",
                        "Regarding your query about a Stanford document discussing context with high accuracy suddenly collapsing in size and its performance decreasing significantly:\n",
                        "\n",
                        "Yes, the web search results verify the existence of such research from Stanford. A LinkedIn post explicitly states, \"**# Claude Code accuracy drops with context collapse: Stanford research**,\" directly addressing the phenomenon of accuracy decreasing with context collapse. Another source from Marktechpost mentions \"Researchers from Stanford University propose a study that explores the impact of accumulating data on model collapse in generative AI models,\" which is related to model performance degradation, though it focuses on data accumulation rather than context size directly.\n",
                        "\n",
                        "**Consistency and Supplementation:**\n",
                        "The information from the PDF, detailing a specific instance of context size collapsing and accuracy dropping, is consistent with the type of phenomenon that Stanford research, as identified in the web search, is investigating. The PDF itself does not state that it is a Stanford document, but the web search successfully verifies that Stanford is indeed involved in research concerning \"context collapse\" and its impact on accuracy. The web information supplements the PDF by confirming that this specific issue is a recognized area of study by Stanford researchers.\n"
                    ]
                }
            ],
            "source": [
                "# Example Usage\n",
                "query = \"Summarize the main points of the report and verify whether there is a document from Stanford which talks about context with high accuracy suddenly collapsing in size and its performance decreasing significantly\"\n",
                "response = verify_information(query)\n",
                "print(\"\\n=== AGENT RESPONSE ===\\n\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "579baacc",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
